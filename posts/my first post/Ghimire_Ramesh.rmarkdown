---
title: "R project "
author: "Ramesh Ghimire"
format: html
editor: visual
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(rpart)
library(rsample)
library(caret)
library(mgcv)

knitr::opts_chunk$set(echo = TRUE)



```


## 


```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Run this code chunk without altering it
# clear the session
rm(list=ls())

# Data is stored in a csv file, the first row contains the variable names. 
# we call our data mydata
mydata<-read.csv ("Data_RLab5.csv", header=TRUE)

# remove lowbirthweight
mydata<-mydata%>%
  select(-lowbirthweight)




```


## 


```{r, echo=TRUE}

# Display the structure of the dataset
str(mydata)


```

```{r, echo=TRUE}

# Summary statistics for numeric variables
summary(mydata)

```

```{r, echo=TRUE}

# Check class/type of each variable
sapply(mydata, class)
```

```{r, echo=TRUE}

# Check factor levels for factor variables
sapply(mydata, function(x) if(is.factor(x)) levels(x))
```

```{r, echo=TRUE}

# Recode variables as needed
mydata$mature <- as.factor(mydata$mature)
mydata$premie <- as.factor(mydata$premie) 

```

```{r, echo=TRUE}

# Confirm classes after recoding
sapply(mydata, class)


```

```{r, echo=TRUE}


library(dplyr)
library(tidyr)

# Identify missing values
sapply(mydata, function(x) sum(is.na(x)))

```

```{r, echo=TRUE}

# Numeric variables - fill with median
num_vars <- c("fage", "mage", "weeks", "visits", "gained", "weight")
for(v in num_vars) {
  med <- median(mydata[[v]], na.rm = TRUE) 
  mydata[[v]][is.na(mydata[[v]])] <- med
}

```

```{r, echo=TRUE}

# Categorical variables - fill with mode 
cat_vars <- c("mature", "premie", "marital", "gender", "habit", "whitemom")
for(v in cat_vars) {
  mod <- names(which.max(table(mydata[[v]])))
  mydata[[v]][is.na(mydata[[v]])] <- mod  
}
```

```{r, echo=TRUE}

# Check if any NAs remain
sapply(mydata, function(x) sum(is.na(x)))
```

```{r}

# Determine variable with highest correlation
cor_mat <- cor(mydata[sapply(mydata, is.numeric)], use="pairwise.complete.obs")
highest_cor <- which.max(abs(cor_mat["weight",]))
names(highest_cor) 

# Plot scatterplot 
target_var <- "weight"
pred_var <- names(highest_cor)

ggplot(mydata, aes_string(x = pred_var, y = target_var)) + 
  geom_point(alpha = 0.5) +
  ggtitle(paste0("Scatterplot of ", target_var, " vs ", pred_var))
```


Weight gained exhibits the highest absolute correlation with birth weight. The graph illustrates a positive correlation, indicating that mothers who gained extra weight during pregnancy tended to have babies with higher weights. Although there are a few outliers, the general trend suggests that weight gain during pregnancy supports fetal development.

## 

------------------------------------------------------------------------

## 


```{r, echo=TRUE}
# Please provide your code for Task 2 in this code chunk
# split the sample by using rsample package

# Split the data into a training set (70%) and a test set (30%)
set.seed(123456)

# Load rsample package
library(rsample)

# Take a 70/30 split stratified on weight 
split <- initial_split(mydata, prop = 0.7, strata = "weight")

# Extract training and test sets
train_data <- training(split) 
test_data <- testing(split)

# Check proportions
prop.table(table(train_data$weight)) 
prop.table(table(test_data$weight))

```

```{r}

split <- initial_split(mydata, prop = 0.7, strata = "weight")

train_data <- training(split)
test_data <- testing(split)
```


## 


```{r, echo=TRUE}
# Please provide your code for Task 3  in this code chunk


# Linear model on training data 
linearmodel <- lm(weight ~ ., data = train_data)

# Make predictions on test data
predicted_weights_ols <- predict(linearmodel, newdata = test_data)

# Calculate MSPE
MSPE_linear <- mean((test_data$weight - predicted_weights_ols)^2)

# Print MSPE
print(MSPE_linear)

```


MSPE_linear = 1.2600889, indicates improved predictions on the unseen test dataset, affirming the efficacy of the model trained on the training dataset.

## 


```{r, echo=TRUE}
# Please provide your code for Task 4 in this code chunk

library(mgcv)

# Fit GAM model
gam_model <- gam(weight ~ s(fage) + s(mage) + s(weeks) + s(visits) + 
                  s(gained) + marital + gender + habit + whitemom, 
                data = train_data,
                method = "REML")

# Print smoothing parameters
print(summary(gam_model)$s.table)


```

```{r}

# Make predictions on test set
predicted_weights_gam <- predict(gam_model, newdata = test_data)

# Calculate MSPE 
MSPE_gam <- mean((test_data$weight - predicted_weights_gam)^2)

# Print MSPE
print(MSPE_gam)
```


MSPE_gam = 1.16755, reflects lower error and improved predictions in the GAM model, attributed to the incorporation of non-linear smoothing functions for continuous predictors.

## 


```{r, echo=TRUE}
# Linear Regression MSPE: MSPE_linear = 1.260889

# GAM Model MSPE: MSPE_gam = 1.16755
```


Linear Regression MSPE: 1.260889

GAM Model MSPE: 1.16755

A lower MSPE signifies enhanced predictive accuracy for the 'weight' variable in the test dataset. In this comparison:

-   If MSPE_linear \< MSPE_gam: Linear regression outperforms in predictive performance.

-   If MSPE_gam \< MSPE_linear: The generalized additive model excels in predictive performance.

-   If MSPE_linear â‰ˆ MSPE_gam: Both models perform similarly in predicting the 'weight' variable.

Analysis reveals the GAM model's superiority with a lower MSPE of 1.117023, indicating reduced deviation between actual and predicted values.

